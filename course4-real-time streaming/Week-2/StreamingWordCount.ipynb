{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfW2gwDt8Grj"
   },
   "source": [
    "# Task 1. Stateful wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1glnG-f8Grk"
   },
   "source": [
    "In this task you're receiving batches in real time through DStream. You need to create the Wordcount program with the saving and updating of the state after each batch. \n",
    "\n",
    "You have to print the TOP-10 most popular words in the input dataset and its quantity. \n",
    "\n",
    "There are several points for this task:\n",
    "\n",
    "1) You have to print the data only at the end. The criteria is if you have received the first empty RDD, the stream is finished. At this moment you have to print the result and stop the context.\n",
    "\n",
    "2) You may split the line with using $flatMap$ method in DStream.\n",
    "\n",
    "3) In this task you need to filter out short words (with length less than 4). For this aim you may use $filter$ method in DStream.\n",
    "\n",
    "4) Remember that you should use string lowercase. You may use $map$ method in DStream to transform words to such case.\n",
    "\n",
    "5) You may use  $reduceByKey$ in Dstream to merge the tuples with the same key by summing the word count value.\n",
    "\n",
    "6) In this task, you need to be able to maintain the state across the batches. You may use the $updateStateByKey()$ method, which provides an access to the state variable and helps you to implement the \"stateful\" approach. You can update the current state with the results of every batch.\n",
    "\n",
    "You may find more useful methods in the following sources:\n",
    "\n",
    "* Book \"Learning Spark: Lightning-Fast Big Data Analysis\" by Holden Karau.\n",
    "\n",
    "* [Spark Streaming documentation](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "* [PySpark Streaming documentation](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-module) \n",
    "\n",
    "* [PySpark Streaming examples](https://github.com/apache/spark/tree/master/examples/src/main/python/streaming)\n",
    "\n",
    "\n",
    "Here you can find the starter for the main steps of the task. You can use other methods to get the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQADoZxB8Grk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FqHOOD0kRJ3"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for emulation realtime batch arriving. But figure out the code, it will help you when you work with real SparkStreaming applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfGtE-J_8Grm"
   },
   "outputs": [],
   "source": [
    "# Preparing SparkContext\n",
    "sc = SparkContext(master='local[4]')\n",
    "\n",
    "# Preparing batches with the input data\n",
    "DATA_PATH = \"/data/wiki/en_articles_streaming\"\n",
    "\n",
    "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
    "\n",
    "# Creating Dstream to emulate realtime data generating\n",
    "BATCH_TIMEOUT = 5  # Timeout between batch generation\n",
    "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
    "dstream = ssc.queueStream(rdds=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b12Ee-mArAD"
   },
   "source": [
    "There are 2 flags used in this task.\n",
    "* The `finished` flag indicates if the current RDD is empty.\n",
    "* The `printed` one indicates that the result has been printed and SparkStreaming context can be stopped.\n",
    "\n",
    "For filtering out punctuation and other junk symbols use this pattern: `re.split(\"\\W*\\s+\\W*\", line.strip(), flags=re.UNICODE)`\n",
    "\n",
    "**NB**. Spark transformations work in a lazy mode. When the transformation is called, it doesn't execute really. It just saves in the computational DAG. All the transformations will be executed when the action will be called. Let's look at `print_only_at_the_end()` function. The action will be called only when the stream will be finished. So in this moment  Spark will execute all the transformations. This will lead to container's overflow if the dataset is really big. So if you faced the error like `Container killed by YARN for exceeding memory limits`, call some action before `if` clause in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yw_69uKz8Gro"
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "printed = False\n",
    "\n",
    "def set_ending_flag(rdd):\n",
    "    global finished\n",
    "    if rdd.isEmpty():\n",
    "        finished = True\n",
    "\n",
    "\n",
    "def print_only_at_the_end(rdd):\n",
    "    global printed\n",
    "    \n",
    "    if finished and not printed:\n",
    "        # Type your code for printing the sorted data from the stream in a loop\n",
    "        res = rdd.take(10)\n",
    "        for i in range(len(res)):\n",
    "            (word, count) = res[i]\n",
    "            print('{}\\t{}'.format(word, count))\n",
    "        printed = True\n",
    "\n",
    "\n",
    "# If we have received an empty rdd, the stream is finished.\n",
    "# So print the result and stop the context.\n",
    "dstream.foreachRDD(set_ending_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81GjQtpA8Grp"
   },
   "outputs": [],
   "source": [
    "# Type your code for data processing and aggregation here\n",
    "\n",
    "def update_func(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "dstream.flatMap(lambda line: re.split(\"\\W*\\s+\\W*\", line.strip().lower(), flags=re.UNICODE))\\\n",
    "    .filter(lambda word: len(word) > 3)\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a+b)\\\n",
    "    .updateStateByKey(update_func)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\\\n",
    "    .foreachRDD(print_only_at_the_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_msKdjZ-pv0A"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for stopping SparkStreaming context and Spark context when the stream finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9pgh8R88Grq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that\t81572\n",
      "with\t79559\n",
      "from\t58201\n",
      "which\t42198\n",
      "this\t38252\n",
      "were\t34403\n",
      "also\t31573\n",
      "have\t29871\n",
      "their\t24579\n",
      "other\t23538\n"
     ]
    }
   ],
   "source": [
    "ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))  # checkpoint for storing current state        \n",
    "ssc.start()\n",
    "while not printed:\n",
    "    pass\n",
    "ssc.stop()  # when the result was printed, stop SparkStreaming context\n",
    "sc.stop()  # stop Spark context to be able restart the code without restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbICM7b9s4wF"
   },
   "source": [
    "Here you can see a part of an output on the sample dataset:\n",
    "\n",
    "```\n",
    "...\n",
    "which 42198\n",
    "this 38252\n",
    "were 34403\n",
    "...\n",
    "```\n",
    "\n",
    "Of course, the numbers may be different but not very much (the error about 2% will be accepted)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1001_flavor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
